<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>BayesFitness · BayesFitness</title><script data-outdated-warner src="assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="assets/documenter.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href>BayesFitness</a></span></div><form class="docs-search" action="search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li class="is-active"><a class="tocitem" href>BayesFitness</a><ul class="internal"><li><a class="tocitem" href="#Example-inference"><span>Example inference</span></a></li><li><a class="tocitem" href="#Contents"><span>Contents</span></a></li></ul></li><li><a class="tocitem" href="mcmc/">mcmc</a></li><li><a class="tocitem" href="model/">model</a></li><li><a class="tocitem" href="stats/">stats</a></li><li><a class="tocitem" href="utils/">utils</a></li><li><a class="tocitem" href="viz/">viz</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>BayesFitness</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>BayesFitness</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/mrazomej/BayesFitness.jl/blob/main/docs/src/index.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="BayesFitness"><a class="docs-heading-anchor" href="#BayesFitness">BayesFitness</a><a id="BayesFitness-1"></a><a class="docs-heading-anchor-permalink" href="#BayesFitness" title="Permalink"></a></h1><p>Welcome to the documentation of <code>BayesFitness.jl</code>! The accompanying paper, <em>Bayesian inference of relative fitness on high-throughput pooled competition assays</em>, explains all of the biological and mathematical background needed to understand this package. Here, we only focus on how to use the package, assuming the user already understands the objective of inferring the posterior probability distribution of the relative fitness of mutant strains in a pooled competition assay.</p><p>The package is divided into modules. Here&#39;s a brief description of the content of each module, but please visit their respective documentations to understand what each module is intended for.</p><ul><li><code>utils</code>: Series of miscellaneous functions that make the data wrangling and processing much simpler.</li><li><code>viz</code>: <a href="https://docs.makie.org/stable/"><code>Makie.jl</code></a>-based module with useful plotting functions to display the data and the MCMC results for visual diagnostics.</li><li><code>stats</code>: Statistical functions used in the inference problem.</li><li><code>model</code>: <a href="https://turing.ml"><code>Turing.jl</code></a>-based Bayesian models used to infer the population mean fitness via the neutral lineages as well as the mutants&#39; relative fitness.</li><li><code>mcmc</code>: The main module with which to perform the Markov-Chain Monte Carlo sampling of the posterior distributions.</li></ul><h2 id="Example-inference"><a class="docs-heading-anchor" href="#Example-inference">Example inference</a><a id="Example-inference-1"></a><a class="docs-heading-anchor-permalink" href="#Example-inference" title="Permalink"></a></h2><p>To get you going with the package, let&#39;s walk through a basic inference pipeline for one competition assay. Our ultimate goal consists of inferring the relative fitness for each of the mutant barcodes. To that end, we assume that the frequency time-series obeys the following equation</p><p class="math-container">\[f_{t+1}^{(b)} = f_{t}^{(b)} \mathrm{e}^{\left(s^{(b)} - \bar{s}_t \right)\tau},
\tag{1}\]</p><p>where <span>$f_{t}^{(b)}$</span> is the frequency of barcode <span>$b$</span> at time <span>$t$</span>, <span>$s^{(b)}$</span> is the relative fitness of this barcode, <span>$\bar{s}_t$</span> is the population mean fitness at time <span>$t$</span>, and <span>$\tau$</span> is the time interval between time <span>$t$</span> and <span>$t+1$</span>.</p><p>The first step consists of importing the necessary packages. </p><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>We use <code>import</code> rather than the more common <code>using</code> command. We find it better to keep the project organized, but feel free to use whatever is more convenient for you!</p></div></div><pre><code class="language-julia hljs"># Import Bayesian inference package
import BayesFitness

# Import libraries to manipulate data
import DataFrames as DF
import CSV</code></pre><p>After having imported the libraries, we need to load our dataset into memory. This dataset is already in the format needed for <code>BayesFitness.jl</code> to work, so we don&#39;t have to modify anything.</p><pre><code class="language-julia hljs"># Import data
data = CSV.read(&quot;~/git/BayesFitness/test/data/data_example_01.csv&quot;, DF.DataFrame)</code></pre><p>Here you will replace <code>&quot;~/git/BayesFitness/test/data&quot;</code> with the directory where your data is stored, and <code>&quot;data_example_01.csv&quot;</code> with the name of the file containing the data. The resulting <code>DataFrame</code> looks something like this:</p><pre><code class="nohighlight hljs">| BCID_x | barcode                                               | name                    | count | time | neutral | count_sum  |
|--------|-------------------------------------------------------|-------------------------|-------|------|---------|------------|
| 0      | TGATCAATCTACAAAAATATTTAATG_GAGTGAAACATGAATGGTATTCATCA | Batch1_1Day-T0_combined | 53    | 0    | FALSE   | 543947     |
| 1      | CCGCCAATCCCGAACCCCGTTTCGCC_ACTCTAACGTGTAACTAATTTTGAGT | Batch1_1Day-T0_combined | 1213  | 0    | FALSE   | 543947     |
| 2      | GACAGAAAAGCCAAATGGATTTACCG_ATGGGAACACGGAATGATCTTTTATT | Batch1_1Day-T0_combined | 17    | 0    | FALSE   | 543947     |
| 3      | CCAACAAAACACAAATCTGTTGTGTA_TACTAAATAAGTAAGGGAATTCTGTT | Batch1_1Day-T0_combined | 19    | 0    | FALSE   | 543947     |
| 4      | TATCGAAACCCAAAGAGATTTAATCG_ATGACAAACTTTAAATAATTTAATTG | Batch1_1Day-T0_combined | 23    | 0    | FALSE   | 543947     |
| 5      | TATCGAAACCCAAAGAGATTTAATCG_CGATCAAAGACTAACTTATTTTGTGG | Batch1_1Day-T0_combined | 16    | 0    | FALSE   | 543947     |
| 6      | TATCGAAACCCAAAGAGATTTAATCG_TTGCCAAGCTGGAAAGCTTTTTATGA | Batch1_1Day-T0_combined | 12    | 0    | FALSE   | 543947     |
| 7      | ATCACAATAACTAAACTGATTCTTCA_CTCATAACATCAAAAAAAATTCAAAT | Batch1_1Day-T0_combined | 161   | 0    | FALSE   | 543947     |
| 8      | TATCGAAACCCAAAGAGATTTAATCG_GTTTAAACCATTAATTATATTAGATC | Batch1_1Day-T0_combined | 19    | 0    | FALSE   | 543947     |</code></pre><p>The relevant columns in this data frame are:</p><ul><li><code>barcode</code>: The unique ID that identifies the barcode.</li><li><code>count</code>: The number of reads for this particular barcode.</li><li><code>time</code>: The time point ID indicating the order in which samples were taken.</li><li><code>neutral</code>: Indicator of whether the barcode belongs to a neutral lineage or not.</li></ul><p>Let&#39;s take a look at the data. The <a href="viz/"><code>BayesFitness.viz</code></a> module has several <a href="https://docs.makie.org/stable/"><code>Makie.jl</code></a>-based functions to easily display the data. Let&#39;s import the necessary plotting libraries</p><pre><code class="language-julia hljs"># Import plotting libraries
using CairoMakie
import ColorSchemes</code></pre><p>First, we plot the barcode frequency trajectories. For this, we use the convenient <a href="viz/#BayesFitness.viz.bc_time_series!-Tuple{Makie.Axis, DataFrames.AbstractDataFrame}"><code>BayesFitness.viz.bc_time_series!</code></a> function.</p><pre><code class="language-julia hljs"># Initialize figure
fig = Figure(resolution=(450, 350))

# Add axis
ax = Axis(
    fig[1, 1],
    xlabel=&quot;time point&quot;,
    ylabel=&quot;barcode frequency&quot;,
    yscale=log10,
    title=&quot;frequency trajectories&quot;
)

# Plot Mutant barcode trajectories with varying colors
BayesFitness.viz.bc_time_series!(
    ax,
    data[.!data.neutral, :];
    quant_col=:freq,
    zero_lim=1E-9,
    zero_label=&quot;extinct&quot;,
    alpha=0.25,
    linewidth=2
)

# Plot Neutral barcode trajectories with a single dark blue color
BayesFitness.viz.bc_time_series!(
    ax,
    data[data.neutral, :];
    quant_col=:freq,
    zero_lim=1E-9,
    color=ColorSchemes.Blues_9[end],
    alpha=0.9,
    linewidth=2
)</code></pre><p>We highlight the neutral barcodes⸺defined to have relative fitness <span>$s^{(n)}=0$</span>⸺with dark blue lines. The rest of the light-color lines correspond to individual barcodes.</p><p><img src="figs/fig01.svg" alt/></p><p>We can rewrite Eq. (1) as</p><p class="math-container">\[\frac{1}{\tau} \ln \frac{f_{t+1}^{(b)}}{f_{t}^{(b)}} = 
\left(s^{(b)} - \bar{s}_t \right).
\tag{2}\]</p><p>In this form, we can se that the relevant quantity we need to infer the values of the population mean fitness <span>$\bar{s}_t$</span> and the barcode relative fitness <span>$s^{(b)}$</span> are not the frequencies themselves, but the log ratio of these frequencies between two adjacent time points. Let&#39;s plot this log frequency ratio using the <a href="viz/#BayesFitness.viz.logfreq_ratio_time_series!-Tuple{Makie.Axis, DataFrames.AbstractDataFrame}"><code>BayesFitness.viz.logfreq_ratio_time_series!</code></a> function.</p><pre><code class="language-julia hljs"># Initialize figure
fig = Figure(resolution=(450, 350))

# Add axis
ax = Axis(
    fig[1, 1],
    xlabel=&quot;time point&quot;,
    ylabel=&quot;ln(fₜ₊₁/fₜ)&quot;,
    title=&quot;log-frequency ratio&quot;
)

# Plot log-frequency ratio of mutants with different colors
BayesFitness.viz.logfreq_ratio_time_series!(
    ax,
    data[.!data.neutral, :];
    freq_col=:freq,
    alpha=0.25,
    linewidth=2
)

# Plot log-frequency ratio of neutrals with a single dark blue color
BayesFitness.viz.logfreq_ratio_time_series!(
    ax,
    data[data.neutral, :];
    freq_col=:freq,
    color=ColorSchemes.Blues_9[end],
    alpha=1.0,
    linewidth=2
)</code></pre><div class="admonition is-success"><header class="admonition-header">Tip</header><div class="admonition-body"><p>We expect is to see these log-frequency ratios as relatively flat lines. Especially for the neutral lineages.</p></div></div><p><img src="figs/fig02.svg" alt/></p><h3 id="Inferring-the-population-mean-fitness"><a class="docs-heading-anchor" href="#Inferring-the-population-mean-fitness">Inferring the population mean fitness</a><a id="Inferring-the-population-mean-fitness-1"></a><a class="docs-heading-anchor-permalink" href="#Inferring-the-population-mean-fitness" title="Permalink"></a></h3><p>With the data in hand, our first task is to infer the population mean fitness using the neutral lineages. For this, we use the <a href="mcmc/#BayesFitness.mcmc.mcmc_mean_fitness-Tuple{}"><code>BayesFitness.mcmc.mcmc_mean_fitness</code></a> function from the <code>mcmc</code> module. The main parameters we need to define are:</p><ul><li><code>:data</code>: Tidy data frame containing the raw barcode counts.</li><li><code>:n_walkers</code>: Number of MCMC chains to run in parallel. NOTE: Having multiple chains run in parallel is convenient for diagnostics. <code>BayesFitness.jl</code> will use the available threads, so make sure you have more than one thread in your <code>julia</code> session if you want to run this inference in a multi-threaded way.</li><li><code>:outputdir</code>: String pointing to the output directory.</li><li><code>outputname</code>: String defining the pattern for the output files. This can be something related to the dataset. For example, the growth media, or the date of the experiment, of whatever metadata used to distinguish different datasets.</li><li><code>model</code>: Bayesian model from the <code>model</code> module that defines the posterior distribution to be sampled.</li><li><code>model_kwargs</code>: The parameters required by the <code>model</code> function.</li></ul><p>We compile all of these parameters into a dictionary that looks something like this:</p><pre><code class="language-julia hljs"># Define dictionary with necessary parameters for MCMC sampling of the 
# population mean fitness
param = Dict(
    :data =&gt; data, 
    :n_walkers =&gt; 3, 
    :n_steps =&gt; 1_000,
    :outputdir =&gt; &quot;./output/&quot;,
    :outputname =&gt; &quot;data_01_meanfitness&quot;,
    :model =&gt; BayesFitness.model.mean_fitness_neutrals_lognormal,
    :model_kwargs =&gt; Dict(
        :α =&gt; BayesFitness.stats.dirichlet_prior_neutral(
            data[data.time.==0, :neutral],
        )
    )
)</code></pre><p>We are now ready to sample the posterior distribution for the population mean fitness. <a href="mcmc/"><code>mcmc</code></a> makes this extremely easy by using the <a href="mcmc/#BayesFitness.mcmc.mcmc_mean_fitness-Tuple{}"><code>BayesFitness.mcmc.mcmc_mean_fitness</code></a> function. All we have to do is run</p><pre><code class="language-julia hljs"># Run inference
BayesFitness.mcmc.mcmc_mean_fitness(; param...)</code></pre><p>The output of this function are <a href="https://github.com/JuliaIO/JLD2.jl"><code>.jld2</code></a> files that save the native data structure with the MCMC samples for each pair of adjacent timepoints. To extract the MCMC samples of the variable we care about⸺equivalent to marginalizing out all the nuisance variables⸺we can use the <a href="utils/#BayesFitness.utils.jld2_concat_chains-Tuple{String, String, Vector{Symbol}}"><code>BayesFitness.utils.jld2_concat_chains</code></a> from the <a href="utils/"><code>utils</code></a> module, indicating the name of the variable we want to extract. What this function does is to search for all <code>.jld2</code> files in the directory that have a particular pattern in their filename, extracts the MCMC samples for the requested variable (the mean fitness in our case) and compiles them into a data frame, where each column represents the variable extracted from each file.</p><pre><code class="language-julia hljs"># Concatenate population mean fitness chains into single chain
chains = BayesFitness.utils.jld2_concat_chains(
    &quot;./output/&quot;, &quot;data_01_meanfitness&quot;, [:sₜ]; id_str=&quot;&quot;
)</code></pre><div class="admonition is-info"><header class="admonition-header">Info</header><div class="admonition-body"><p>Make sure to check all of the functions in the <a href="utils/"><code>BayesFitness.utils</code></a> module that help you extract the information from the MCMC samples.</p></div></div><p>To diagnose the inference of these population mean fitness values, it is useful to plot both the MCMC traces for each walker as well as the resulting density plots. To do this, we feed the <code>chains</code>  data structure to the <a href="viz/#BayesFitness.viz.mcmc_trace_density!-Tuple{GridLayoutBase.GridLayout, MCMCChains.Chains}"><code>BayesFitness.viz.mcmc_trace_density!</code></a> function to automatically generate these plots.</p><pre><code class="language-julia hljs"># Initialize figure
fig = Figure(resolution=(600, 600))

# Generate mcmc_trace_density! plot
BayesFitness.viz.mcmc_trace_density!(fig, chains; alpha=0.5)</code></pre><div class="admonition is-success"><header class="admonition-header">Tip</header><div class="admonition-body"><p>What we want to see from these plots is that the traces all look relatively similar, with no big gaps where the walker got stuck. Furthermore, we want to see that all the densities converged to very similar-looking distributions. That is indeed the case for our dataset. Moreover, the densities look fairly symmetric, so should be able to parametrize the resulting posterior distribution as a Gaussian for our next step in the inference.</p></div></div><p><img src="figs/fig03.svg" alt/></p><p>Another way of assessing the output of this inference step is to plot the posterior predictive checks against the data. The logic behind the posterior predictive checks is the following: before performing the inference on the parameters we seek to learn form the data, we have a prior belief of what those values can be encoded in our prior distribution. We update this prior belief after observing the experimental data given our likelihood function that captures our model for the data generation process. Thus, the posterior distribution of the parameter values contains our updated belief for what the parameter values can be. Therefore, we can sample out of this parameter posterior distribution and feed such parameters to our likelihood function to generate synthetic data. The expectation is that this simulated data should capture the range of experimental data we observed if the model and the inferred parameters describe the data generation process.</p><p>For this particular case of the population mean fitness, we can use the <a href="stats/#BayesFitness.stats.logfreq_ratio_mean_ppc-Tuple{DataFrames.AbstractDataFrame, Int64}"><code>BayesFitness.stats.logfreq_ratio_mean_ppc</code></a> from the <a href="stats/"><code>stats</code></a> module to compute the posterior predictive checks. What this function does is to generate samples for the log-frequency ratios used to infer the population mean fitness values.</p><div class="admonition is-info"><header class="admonition-header">Info</header><div class="admonition-body"><p>Note that the <a href="stats/#BayesFitness.stats.logfreq_ratio_mean_ppc-Tuple{DataFrames.AbstractDataFrame, Int64}"><code>BayesFitness.stats.logfreq_ratio_mean_ppc</code></a> function has methods to work with either <code>MCMCChains.Chains</code> objects or with tidy <code>DataFrames.DataFrame</code>. This allows you to use the data structure you are more comfortable working with.</p></div></div><pre><code class="language-julia hljs"># Name of variables to be extracted from the output MCMC chains on multiple 
# .jld2 files
chain_vars = [:sₜ, :σₜ]

# Extract variables into single chain object
chains = BayesFitness.utils.jld2_concat_chains(
    param[:outputdir], param[:outputname], chain_vars; id_str=&quot;&quot;
)

# Define number of posterior predictive check samples
n_ppc = 5_000

# Define dictionary with corresponding parameters for variables needed for the
# posterior predictive checks
param = Dict(
    :population_mean_fitness =&gt; :sₜ,
    :population_std_fitness =&gt; :σₜ,
)

# Compute posterior predictive checks
ppc_mat = BayesFitness.stats.logfreq_ratio_mean_ppc(
    chains, n_ppc; param=param
)</code></pre><p>Once we generate these samples, we can plot the quantiles of the simulated data with different shades. The <a href="viz/#BayesFitness.viz.ppc_time_series!-Tuple{Makie.Axis, Vector{&lt;:AbstractFloat}, Matrix{&lt;:Real}}"><code>BayesFitness.viz.ppc_time_series!</code></a> function from the <a href="viz/"><code>viz</code></a> module makes this plotting really simple. Let us plot the standard 68-95-97.5 percentiles with different shades of blue and then add the data on top of these shaded areas</p><div class="admonition is-success"><header class="admonition-header">Tip</header><div class="admonition-body"><p>What we expect from this plot is to see that most of the experimental data falls within the range of the simulated data, meaning that the model and the inferred parameters can reproduce the range of our observations.</p></div></div><pre><code class="language-julia hljs"># Initialize figure
fig = Figure(resolution=(450, 350))

# Add axis
ax = Axis(
    fig[1, 1],
    xlabel=&quot;time point&quot;,
    ylabel=&quot;ln(fₜ₊₁/fₜ)&quot;,
    title=&quot;log-frequency ratio PPC&quot;
)

# Define quantiles to compute
qs = [0.68, 0.95, 0.997]

# Define range of colors for each quantile colors
colors = get(ColorSchemes.Blues_9, LinRange(0.25, 0.75, length(qs)))

# Plot posterior predictive check quantiles
BayesFitness.viz.ppc_time_series!(
    ax, qs, ppc_mat; colors=colors
)

# Add plot for median (we use the 5 percentile to have a &quot;thicker&quot; line showing
# the median)
BayesFitness.viz.ppc_time_series!(
    ax, [0.05], ppc_mat; colors=ColorSchemes.Blues_9[end:end]
)

# Plot log-frequency ratio of neutrals
BayesFitness.viz.logfreq_ratio_time_series!(
    ax,
    data[data.neutral, :];
    freq_col=:freq,
    color=:black,
    alpha=1.0,
    linewidth=2
)</code></pre><p><img src="figs/fig04.svg" alt/></p><p>This plot shows that the range of inferred population mean fitnesses does capture the log-frequency ratios of the neutral lineages. Therefore, we can confidently move to the next stage of our inference pipeline.</p><h3 id="Inferring-mutants&#39;-relative-fitness"><a class="docs-heading-anchor" href="#Inferring-mutants&#39;-relative-fitness">Inferring mutants&#39; relative fitness</a><a id="Inferring-mutants&#39;-relative-fitness-1"></a><a class="docs-heading-anchor-permalink" href="#Inferring-mutants&#39;-relative-fitness" title="Permalink"></a></h3><p>Once we make sure that the population mean fitness looks okay, we can tackle the inference of each mutant relative fitness. The process is very similar, the main difference being that we use the results from the previous step as part of the inputs that go into the corresponding Bayesian model defined in the <a href="model/"><code>model</code></a> module. More specifically, the inferred population mean fitness enters our inference as a &quot;prior&quot; on this value. However, we cannot feed the raw output of the MCMC samples we obtained from the previous step as a prior; this prior distribution has to be parametrized. Since our density plots look fairly symmetric, we assume we can parametrize the population mean fitness values as a Gaussian distribution. Thus, we need to fit a Gaussian distribution for each MCMC chain sampled in the previous section. The <a href="stats/#BayesFitness.stats.gaussian_prior_mean_fitness-Tuple{DataFrames.AbstractDataFrame}"><code>BayesFitness.stats.gaussian_prior_mean_fitness</code></a> function in the <a href="stats/"><code>stats</code></a> module can help us with this.</p><div class="admonition is-info"><header class="admonition-header">Info</header><div class="admonition-body"><p>Note that the <a href="stats/#BayesFitness.stats.gaussian_prior_mean_fitness-Tuple{DataFrames.AbstractDataFrame}"><code>BayesFitness.stats.gaussian_prior_mean_fitness</code></a> function has methods to work with either <code>MCMCChains.Chains</code> objects or with tidy <code>DataFrames.DataFrame</code>. This allows you to use the data structure you are more comfortable working with.</p></div></div><pre><code class="language-julia hljs"># Extract mean fitness MCMC chains
mean_fitness_chains = BayesFitness.utils.jld2_concat_chains(
    &quot;./output/&quot;, &quot;data_01_meanfitness&quot;, [:sₜ]; id_str=&quot;&quot;
)

# Infer mean fitness distribution parameters by fitting a Gaussian
mean_fitness_dist = BayesFitness.stats.gaussian_prior_mean_fitness(
    mean_fitness_chains
)</code></pre><p>To assess whether assuming a parametrized Gaussian distribution can described the MCMC chains from the previous section, we need to visually compare both distributions. An effective way to do this is to compare the empirical cumulative distribution function (ecdf) built from the MCMC samples with the parametric cumulative distribution function (cdf) we obtain from fitting the Gaussian distributions. The <a href="viz/#BayesFitness.viz.mcmc_fitdist_cdf!-Tuple{Makie.Axis, Vector{&lt;:Real}, Distributions.Distribution{Distributions.Univariate, Distributions.Continuous}}"><code>BayesFitness.viz.mcmc_fitdist_cdf!</code></a> function from does exactly this. Let&#39;s plot this comparison for all four inferred population mean fitness values from the previous section</p><pre><code class="language-julia hljs"># Infer mean fitness distributions by fitting a Gaussian
fit_dists = BayesFitness.stats.gaussian_prior_mean_fitness(
    mean_fitness_chains,
    params=false
)

# Initialize figure
fig = Figure(resolution=(600, 600))

# Add axis objects for each timepoint
axes = [
    Axis(
        fig[i, j],
        xlabel=&quot;population mean fitness (s̄ₜ)&quot;,
        ylabel=&quot;ecdf&quot;,
    ) for i = 1:2 for j = 1:2
]

# Loop through time points
for (i, var) in enumerate(names(mean_fitness_chains))
    # Plot ECDF
    BayesFitness.viz.mcmc_fitdist_cdf!(
        axes[i],
        Array(mean_fitness_chains[var])[:],
        fit_dists[i]
    )

    axes[i].title = &quot;timepoint $(i)&quot;
end # for</code></pre><p><img src="figs/fig05.svg" alt/></p><div class="admonition is-success"><header class="admonition-header">Tip</header><div class="admonition-body"><p>If the population mean fitness can be parametrized as a Gaussian distribution, there should be minimal differences between the <code>mcmc</code> chain and the fit distribution as shown in the figure above.</p></div></div><p>We can now define the dictionary containing the parameters that go into the [<code>BayesFitness.mcmc.mcmc_mutant_fitness</code>] function from the <code>mcmc</code> module. </p><div class="admonition is-success"><header class="admonition-header">Tip</header><div class="admonition-body"><p>The <a href="mcmc/#BayesFitness.mcmc.mcmc_mutant_fitness-Tuple{}"><code>BayesFitness.mcmc.mcmc_mutant_fitness</code></a> function has two multi-threading modalities: <code>multithread_chain</code> and <code>multithread_mutant</code>. Only one of them can be active at any moment. The <code>multithread_chain</code>  samples multiple chains <strong>for a single mutant at the time</strong> in a multithread fashion. The <code>multithread_mutant</code> samples <strong>multiple mutants at the time</strong>, with one chain per thread.</p><p>On the one hand, if you only have ≈ 4 threads available in your computer, you might want to use <code>multithread_chain</code> to sample 3-4 chains per mutant in parallel. On the other hand, if you have &gt; 4 threads available, running  <code>multithread_mutant</code> can significantly speedup your computation; especially if you have ≥ 8 threads available.</p></div></div><pre><code class="language-julia hljs"># Define function parameters
param = Dict(
    :data =&gt; data,
    :n_walkers =&gt; 3,
    :n_steps =&gt; 1_000,
    :outputdir =&gt; &quot;./output/&quot;,
    :outputname =&gt; &quot;data_01_mutantfitness&quot;,
    :model =&gt; BayesFitness.model.mutant_fitness_lognormal,
    :model_kwargs =&gt; Dict(
        :α =&gt; BayesFitness.stats.beta_prior_mutant(
            data[data.time.==0, :barcode],
        ),
        :μ_s̄ =&gt; mean_fitness_dist[1],
        :σ_s̄ =&gt; mean_fitness_dist[2],
    ),
    :multithread_mutant =&gt; true,
)</code></pre><p>Finally, we run the inference.</p><pre><code class="language-julia hljs"># Run inference in a multithread fashion
BayesFitness.mcmc.mcmc_mutant_fitness(; param...)</code></pre><p>This function generates <code>.jld2</code> files with the MCMC chains for each of the mutant barcodes. Let&#39;s look at an example inference. We will specifically load the inference for the barcode that reached the highest count throughout the experiment. For this, we use the <code>Glob.jl</code> library to locate the corresponding <code>.jld2</code> file. <code>.jld2</code> files store multiple Julia objects as dictionary. In our case, a single object named <code>&quot;chain&quot;</code> was saved on this file, so when loading the file, we index this single object.</p><pre><code class="language-julia hljs"># Find barcode with maximum count
bc = data[first(argmax(data.count, dims=1)), :barcode]

# Select file to process
file = first(
    Glob.glob(&quot;$(param[:outputdir])/$(param[:outputname])*$(bc).jld2&quot;)
)

# Load one of the files as an example
chain = JLD2.load(file)[&quot;chain&quot;]</code></pre><p>As a first diagnostic, let&#39;s look at the trace and density plots for the mutant fitness and the likelihood standard deviation.</p><pre><code class="language-julia hljs"># Name variables to be extracted from chains
chain_vars = [Symbol(&quot;s⁽ᵐ⁾&quot;), Symbol(&quot;σ⁽ᵐ⁾&quot;)]

# Extract variables from chain
chn = chain[chain_vars]

# Initialize figure
fig = Figure(resolution=(600, 350))

# Generate mcmc_trace_density! plot
BayesFitness.viz.mcmc_trace_density!(fig, chn; alpha=0.5)</code></pre><p><img src="figs/fig06.svg" alt/></p><p>We can see that the mutant fitness is centered around 1.5. But the Bayesian analysis gives us a principled way to estimate the uncertainty on this estimate.</p><p>To make sure the inferred fitness value agrees with the experimental data, we must generate posterior predictive (retrodictive) checks. For this we need to extract the necessary information from the <code>mcmc_chain</code> object. In particular, we are interested in the mcmc traces for the mutant fitness (named <code>:s⁽ᵐ⁾</code> in the chain), all of the population mean fitness traces (named <code>:s̲ₜ[i]</code> in the chain, where <code>i</code> indexes the time point), the standard deviation for the mutant fitness (named <code>:σ⁽ᵐ⁾</code> in the chain), and the initial barcode frequency (named <code>:f̲⁽ᵐ⁾[1]</code> in the chain).</p><pre><code class="language-julia hljs"># Name variables to be extracted from chains
chain_vars = [Symbol(&quot;s⁽ᵐ⁾&quot;), Symbol(&quot;σ⁽ᵐ⁾&quot;), Symbol(&quot;f̲⁽ᵐ⁾[1]&quot;), :s̲ₜ]

# Locate variable names to extract from chain and append them into a single 
# vector
chain_names = reduce(
    vcat, [MCMCChains.namesingroup(chain, var) for var in chain_vars]
)

# Extract chain variables
chn = chain[chain_names]</code></pre><p>We also need to extract the actual measurements for the specific barcode we are looking as an example. This can easily be extracted from our tidy data frame.</p><pre><code class="language-julia hljs"># Extract data for barcode example
data_bc = data[data.barcode.==bc, :]

# Sort data by time
DF.sort!(data_bc, :time)</code></pre><p>With these variables in hand, we can generate some posterior predictive checks for the barcode frequency trajectories. To generate these samples we use the <a href="stats/#BayesFitness.stats.freq_mutant_ppc-Tuple{DataFrames.AbstractDataFrame, Int64}"><code>BayesFitness.stats.freq_mutant_ppc</code></a> function that takes a chain (or a tidy dataframe) as an input with MCMC chains for the following variables:</p><ul><li>The mutant mean fitness <span>$s^{(m)}$</span>.</li><li>The standard deviation from the mutant fitness inference likelihood function <span>$\sigma^{(m)}$</span>.</li><li>The initial frequency for the mutant <span>$f^{(m)}_0$</span>.</li><li>The corresponding population mean fitness values for each time point where the data was taken <span>$\underline{\bar{s}}_t$</span>.</li></ul><pre><code class="language-julia hljs"># Define number of posterior predictive check samples
n_ppc = 5_000

# Define dictionary with corresponding parameters for variables needed for the
# posterior predictive checks
param = Dict(
    :mutant_mean_fitness =&gt; :s⁽ᵐ⁾,
    :mutant_std_fitness =&gt; :σ⁽ᵐ⁾,
    :mutant_freq =&gt; Symbol(&quot;f̲⁽ᵐ⁾[1]&quot;),
    :population_mean_fitness =&gt; :s̲ₜ,
)

# Compute posterior predictive checks
ppc_mat = BayesFitness.stats.freq_mutant_ppc(
    chn,
    n_ppc;
    param=param
)</code></pre><p>Next, we can use the <a href="viz/#BayesFitness.viz.ppc_time_series!-Tuple{Makie.Axis, Vector{&lt;:AbstractFloat}, Matrix{&lt;:Real}}"><code>BayesFitness.viz.ppc_time_series!</code></a> function to plot these posterior predictive checks to compare it with the frequency trajectory measured experimentally.</p><pre><code class="language-julia hljs"># Initialize figure
fig = Figure(resolution=(450, 350))

# Add axis
ax = Axis(
    fig[1, 1],
    xlabel=&quot;time point&quot;,
    ylabel=&quot;barcode frequency&quot;,
    title=&quot;frequency trajectories&quot;,
    yscale=log10,
)

# Define quantiles to compute
qs = [0.95, 0.675]

# Define colors
colors = get(ColorSchemes.Blues_9, LinRange(0.5, 0.75, length(qs)))

# Plot posterior predictive checks
BayesFitness.viz.ppc_time_series!(
    ax, qs, ppc_mat; colors=colors
)

# Add plot for median
BayesFitness.viz.ppc_time_series!(
    ax, [0.03], ppc_mat; colors=ColorSchemes.Blues_9[end:end]
)

# Add scatter of data
scatterlines!(ax, data_bc.freq, color=:black)</code></pre><p><img src="figs/fig07.svg" alt/></p><div class="admonition is-success"><header class="admonition-header">Tip</header><div class="admonition-body"><p>In the previous plot, the credible region expands as time progresses in the experiment. This is to be expected as the uncertainty is propagated over time with new sources of uncertainty added at each time point. But notice that the experimental frequency trajectory falls within the highest probability region.</p></div></div><p>Another way to visualize the agreement between our inferred parameters and the data is to look at the quantity used directly on the inference of the fitness value, i.e., the log frequency ratio between two adjacent time points. For this we can use the <a href="stats/#BayesFitness.stats.logfreq_ratio_mutant_ppc-Tuple{DataFrames.AbstractDataFrame, Int64}"><code>BayesFitness.stats.logfreq_ratio_mutant_ppc</code></a> function to compute the posterior predictive checks. The difference between this function and the previously used <a href="stats/#BayesFitness.stats.freq_mutant_ppc-Tuple{DataFrames.AbstractDataFrame, Int64}"><code>BayesFitness.stats.freq_mutant_ppc</code></a> is that the log frequency ratio function does not require the inferred initial frequency of the mutant as an input since we are only looking at the log log frequency ratio.</p><pre><code class="language-julia hljs"># Name variables to be extracted from chains
chain_vars = [Symbol(&quot;s⁽ᵐ⁾&quot;), Symbol(&quot;σ⁽ᵐ⁾&quot;), :s̲ₜ]

# Locate variable names to extract from chain
chain_names = reduce(
    vcat, [MCMCChains.namesingroup(chain, var) for var in chain_vars]
)

# Extract chain variables
chn = chain[chain_names]

# Define number of posterior predictive check samples
n_ppc = 5_000

# Define dictionary with corresponding parameters for variables needed for the
# posterior predictive checks
param = Dict(
    :mutant_mean_fitness =&gt; :s⁽ᵐ⁾,
    :mutant_std_fitness =&gt; :σ⁽ᵐ⁾,
    :population_mean_fitness =&gt; :s̲ₜ,
)

# Compute posterior predictive checks
ppc_mat = BayesFitness.stats.logfreq_ratio_mutant_ppc(
    chn, n_ppc; param=param
)</code></pre><p>Again, we can use the <a href="viz/#BayesFitness.viz.ppc_time_series!-Tuple{Makie.Axis, Vector{&lt;:AbstractFloat}, Matrix{&lt;:Real}}"><code>BayesFitness.viz.ppc_time_series!</code></a> function to plot the quantiles for our posterior predictive checks.</p><pre><code class="language-julia hljs"># Initialize figure
fig = Figure(resolution=(450, 350))

# Add axis
ax = Axis(
    fig[1, 1],
    xlabel=&quot;time point&quot;,
    ylabel=&quot;ln(fₜ₊₁/fₜ)&quot;,
    title=&quot;log-frequency ratio PPC&quot;
)

# Define quantiles to compute
qs = [0.95, 0.675]

# Define colors
colors = get(ColorSchemes.Blues_9, LinRange(0.5, 0.75, length(qs)))

# Plot posterior predictive checks
BayesFitness.viz.ppc_time_series!(
    ax, qs, ppc_mat; colors=colors
)

# Add plot for median (we use the 5 percentile to have a &quot;thicker&quot; line showing
# the median)
BayesFitness.viz.ppc_time_series!(
    ax, [0.05], ppc_mat; colors=ColorSchemes.Blues_9[end:end]
)

# Add scatter of data
scatterlines!(ax, diff(log.(data_bc.freq)), color=:black)</code></pre><p><img src="figs/fig08.svg" alt/></p><p>We can see that indeed the recovered fitness value greatly agrees with the data.</p><p>This concludes the example inference pipeline. We invited you to explore more the potential in the package and please send any comments/requests through the GitHub repository issues.</p><h2 id="Contents"><a class="docs-heading-anchor" href="#Contents">Contents</a><a id="Contents-1"></a><a class="docs-heading-anchor-permalink" href="#Contents" title="Permalink"></a></h2><ul><li><a href="#BayesFitness">BayesFitness</a></li><li class="no-marker"><ul><li><a href="#Example-inference">Example inference</a></li><li><a href="#Contents">Contents</a></li></ul></li><li><a href="mcmc/#mcmc">mcmc</a></li><li><a href="model/#model">model</a></li><li><a href="stats/#stats">stats</a></li><li><a href="utils/#utils">utils</a></li><li><a href="viz/#viz">viz</a></li></ul></article><nav class="docs-footer"><a class="docs-footer-nextpage" href="mcmc/">mcmc »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.24 on <span class="colophon-date" title="Monday 5 June 2023 21:17">Monday 5 June 2023</span>. Using Julia version 1.8.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
